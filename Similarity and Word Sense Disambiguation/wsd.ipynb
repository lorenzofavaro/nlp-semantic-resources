{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import semcor, wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import Tree\n",
    "import nltk\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/stop_words_FULL.txt') as f:\n",
    "    stop_words = {line for line in f.read().splitlines()}\n",
    "    \n",
    "sentences = semcor.sents()\n",
    "tagged_sentences = semcor.tagged_sents(tag='sem')\n",
    "MAX_INDEX = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_index(evaluated_indices):\n",
    "    while True:\n",
    "        index = random.randrange(MAX_INDEX)\n",
    "        if index not in evaluated_indices:\n",
    "            return index\n",
    "\n",
    "\n",
    "def get_random_word(tagged_sentence, pos):\n",
    "    words = []\n",
    "    for word in tagged_sentence:\n",
    "        if type(word) is Tree and type(word.label()) != str and word.label().synset().pos() == pos:\n",
    "            words.append(' '.join(word.leaves()))\n",
    "    \n",
    "    while words:\n",
    "        word = random.choice(words)\n",
    "        if word not in stop_words and wordnet.synsets(word):\n",
    "            return word.lower()\n",
    "        words.remove(word)\n",
    "        \n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(sentence):\n",
    "    return set(remove_stopwords(tokenize_sentence(remove_punctuation(sentence))))\n",
    "\n",
    "\n",
    "def remove_stopwords(words_list):\n",
    "    return [value.lower() for value in words_list if value.lower() not in stop_words]\n",
    "\n",
    "\n",
    "def tokenize_sentence(sentence):\n",
    "    words_list = []\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    for tag in nltk.pos_tag(word_tokenize(sentence)):\n",
    "        if (tag[1][:2] == \"NN\"):\n",
    "            words_list.append(lmtzr.lemmatize(tag[0], pos=wordnet.NOUN))\n",
    "        elif (tag[1][:2] == \"VB\"):\n",
    "             words_list.append(lmtzr.lemmatize(tag[0], pos=wordnet.VERB))\n",
    "        elif (tag[1][:2] == \"RB\"):\n",
    "             words_list.append(lmtzr.lemmatize(tag[0], pos=wordnet.ADV))\n",
    "        elif (tag[1][:2] == \"JJ\"):\n",
    "             words_list.append(lmtzr.lemmatize(tag[0], pos=wordnet.ADJ))\n",
    "    return words_list\n",
    "\n",
    "# remove punctuation and multiple spaces\n",
    "def remove_punctuation(sentence):\n",
    "    return re.sub('\\s\\s+', ' ', re.sub(r'[^\\w\\s]', '', sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesk Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_signature(sense):\n",
    "    signature = set()\n",
    "    for word in pre_processing(sense.definition()):\n",
    "        signature.add(word)\n",
    "    for example in sense.examples():\n",
    "        for word in pre_processing(example):\n",
    "            signature.add(word)\n",
    "    return signature\n",
    "\n",
    "\n",
    "def lesk_algorithm(word, sentence, pos=None):\n",
    "    best_sense = wordnet.synsets(word)[0]\n",
    "    max_overlap = 0\n",
    "    context = pre_processing(sentence)\n",
    "    synsets = wordnet.synsets(word, pos=pos)\n",
    "        \n",
    "    for sense in synsets:\n",
    "        signature = get_signature(sense)\n",
    "        overlap = len(list(signature & context))\n",
    "        if overlap > max_overlap:\n",
    "            max_overlap = overlap\n",
    "            best_sense = sense\n",
    "    return best_sense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disambiguation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluated_indices = set()\n",
    "max_runs = 10\n",
    "max_k = 50\n",
    "\n",
    "for i in range(max_k):\n",
    "\n",
    "    while True:\n",
    "        index = get_random_index(evaluated_indices)\n",
    "        word = get_random_word(tagged_sentences[index], wordnet.NOUN)\n",
    "        evaluated_indices.add(index)\n",
    "        if word:\n",
    "            break\n",
    "\n",
    "    sentence = sentences[index]\n",
    "    best_sense = lesk_algorithm(word, ' '.join(word for word in sentence), pos=wordnet.NOUN)\n",
    "    \n",
    "    print(f'Sentence: \"{\" \".join(sentence)}\"')\n",
    "    print(f'Word: {word.upper()}')\n",
    "    print(f'Best sense: \"{str(best_sense)} - {best_sense.definition()}\"\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "404b64e01f57285674b9751162c2ed4527d694a9d1b08ef777de9504d46378cb"
  },
  "kernelspec": {
   "display_name": "Python 3.8.6 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
