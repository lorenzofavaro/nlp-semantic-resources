{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import semcor, wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import Tree\n",
    "import nltk\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/stop_words_FULL.txt') as f:\n",
    "    stop_words = {line for line in f.read().splitlines()}\n",
    "    \n",
    "sentences = semcor.sents()\n",
    "tagged_sentences = semcor.tagged_sents(tag='sem')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_index(evaluated_indices, max_index):\n",
    "    while True:\n",
    "        index = random.randrange(max_index)\n",
    "        if index not in evaluated_indices:\n",
    "            return index\n",
    "\n",
    "\n",
    "def get_random_word(tagged_sentence, pos):\n",
    "    words = []\n",
    "    for word in tagged_sentence:\n",
    "        if type(word) is Tree and type(word.label()) != str and word.label().synset().pos() == pos:\n",
    "            words.append(' '.join(word.leaves()))\n",
    "    \n",
    "    while words:\n",
    "        word = random.choice(words)\n",
    "        if word not in stop_words and len(wordnet.synsets(word)) > 0:\n",
    "            return word.lower()\n",
    "        words.remove(word)\n",
    "        \n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(sentence):\n",
    "    return set(remove_stopwords(tokenize_sentence(remove_punctuation(sentence))))\n",
    "\n",
    "\n",
    "def remove_stopwords(words_list):\n",
    "    return [value for value in words_list if value not in stop_words]\n",
    "\n",
    "\n",
    "def tokenize_sentence(sentence):\n",
    "    words = []\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    for tag in nltk.pos_tag(word_tokenize(sentence)):\n",
    "        word = None\n",
    "        if (tag[1][:2] == \"NN\"):\n",
    "            word = lmtzr.lemmatize(tag[0], pos=wordnet.NOUN)\n",
    "        elif (tag[1][:2] == \"VB\"):\n",
    "            word = lmtzr.lemmatize(tag[0], pos=wordnet.VERB)\n",
    "        elif (tag[1][:2] == \"RB\"):\n",
    "            word = lmtzr.lemmatize(tag[0], pos=wordnet.ADV)\n",
    "        elif (tag[1][:2] == \"JJ\"):\n",
    "            word = lmtzr.lemmatize(tag[0], pos=wordnet.ADJ)\n",
    "        \n",
    "        if word:\n",
    "            words.append(word.lower())\n",
    "    return words\n",
    "\n",
    "# remove punctuation and multiple spaces\n",
    "def remove_punctuation(sentence):\n",
    "    return re.sub('\\s\\s+', ' ', re.sub(r'[^\\w\\s]', '', sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesk Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_signature(sense):\n",
    "    signature = set()\n",
    "    sentence_list = [sense.definition()] + sense.examples()\n",
    "\n",
    "    for sentence in sentence_list:\n",
    "        for word in pre_processing(sentence):\n",
    "            signature.add(word)\n",
    "    return signature\n",
    "\n",
    "\n",
    "def get_word_sense(noun, tagged_sentence):\n",
    "    for word in tagged_sentence:\n",
    "        if type(word) is Tree and word[0] == noun:\n",
    "            return word.label().synset()\n",
    "    return None\n",
    "\n",
    "\n",
    "def lesk_algorithm(word, sentence, pos=None):\n",
    "    best_sense = wordnet.synsets(word)[0]\n",
    "    max_overlap = 0\n",
    "    context = pre_processing(sentence)\n",
    "    synsets = wordnet.synsets(word, pos=pos)\n",
    "        \n",
    "    for sense in synsets:\n",
    "        signature = get_signature(sense)\n",
    "        overlap = len(list(signature & context))\n",
    "        if overlap > max_overlap:\n",
    "            max_overlap = overlap\n",
    "            best_sense = sense\n",
    "            \n",
    "    return best_sense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disambiguation (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_k = 50\n",
    "max_index = 3000\n",
    "evaluated_indices = set()\n",
    "predictions = list()\n",
    "reference = list()\n",
    "\n",
    "for i in range(max_k):\n",
    "    while True:\n",
    "        index = get_random_index(evaluated_indices, max_index)\n",
    "        word = get_random_word(tagged_sentences[index], wordnet.NOUN)\n",
    "        evaluated_indices.add(index)\n",
    "        if word:\n",
    "            break\n",
    "\n",
    "    sentence = sentences[index]\n",
    "    best_sense = lesk_algorithm(word, ' '.join(word for word in sentence), pos=wordnet.NOUN)\n",
    "    target_sense = get_word_sense(word, tagged_sentences[index])\n",
    "    \n",
    "    predictions.append(best_sense)\n",
    "    reference.append(target_sense)\n",
    "    \n",
    "    print(f'Sentence: \"{\" \".join(sentence)}\"')\n",
    "    print(f'Word: {word.upper()}')\n",
    "    print(f'Best sense: \"{str(best_sense)} - {best_sense.definition()}\"\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correctly_evaluated = [target for (prediction, target) in zip(predictions, reference) if prediction is target]\n",
    "accuracy = len(correctly_evaluated) / len(reference)\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disambiguation (2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_runs = 10\n",
    "max_k = 50\n",
    "max_index = 7000\n",
    "run_predictions = list()\n",
    "run_reference = list()\n",
    "\n",
    "for run in range(max_runs):\n",
    "    evaluated_indices = set()\n",
    "    predictions = list()\n",
    "    reference = list()\n",
    "\n",
    "    for i in range(max_k):\n",
    "        while True:\n",
    "            index = get_random_index(evaluated_indices, max_index)\n",
    "            word = get_random_word(tagged_sentences[index], wordnet.NOUN)\n",
    "            evaluated_indices.add(index)\n",
    "            if word:\n",
    "                break\n",
    "\n",
    "        sentence = sentences[index]\n",
    "        best_sense = lesk_algorithm(word, ' '.join(word for word in sentence), pos=wordnet.NOUN)\n",
    "        target_sense = get_word_sense(word, tagged_sentences[index])\n",
    "        \n",
    "        predictions.append(best_sense)\n",
    "        reference.append(target_sense)\n",
    "    \n",
    "    run_predictions.append(predictions)\n",
    "    run_reference.append(reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_list = list()\n",
    "\n",
    "for (prediction, reference) in zip(run_predictions, run_reference):\n",
    "    correctly_evaluated = [target for (prediction, target) in zip(prediction, reference) if prediction is target]\n",
    "    accuracy_list.append(len(correctly_evaluated) / len(reference))\n",
    "\n",
    "print(f'Total executions: {max_runs}')\n",
    "print(f'Accuracy list: {accuracy_list}')\n",
    "print(f'Average accuracy: {sum(accuracy_list) / len(accuracy_list)}')\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "404b64e01f57285674b9751162c2ed4527d694a9d1b08ef777de9504d46378cb"
  },
  "kernelspec": {
   "display_name": "Python 3.8.6 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
